<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<style type="text/css">
.styleTittle {
	FONT-SIZE: 45px; FONT-FAMILY: "Arial", Times, serif;
}
.STYLEAuthor {
	FONT-SIZE: 20px; FONT-FAMILY: "Arial", Times, serif
}
.styleAffiliation {
	FONT-SIZE: 16px; FONT-FAMILY: "Arial", Times, serif
}
.STYLESection {
	FONT-SIZE: 30px; FONT-FAMILY: "Arial", Times, serif; FONT-WEIGHT: bold
}
.styleAbstract {
	FONT-SIZE: 20px; FONT-FAMILY: "Arial";
}
.STYLECaption {
	FONT-SIZE: 16px; FONT-FAMILY: "Arial"
}
.STYLECitation {
	FONT-SIZE: 17px; FONT-FAMILY: "Arial"
}
BODY {
	BACKGROUND-IMAGE: none
}
HR {
	BORDER-TOP: purple 1px solid; BORDER-RIGHT: purple 1px solid; BORDER-BOTTOM: purple 1px solid; BORDER-LEFT: purple 1px solid
}
SPAN.style231{
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
SPAN.style2311 {
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
SPAN.style23111 {
	FONT-FAMILY: "Times New Roman","serif"; mso-style-name: style231; mso-style-unhide: no; mso-ansi-font-size: 12.0pt; mso-bidi-font-size: 12.0pt; mso-ascii-font-family: "Times New Roman"; mso-hansi-font-family: "Times New Roman"; mso-bidi-font-family: "Times New Roman"
}
.style47 {
	FONT-SIZE: 16pt; COLOR: #000000
}
.style55 {
	FONT-SIZE: 16px; FONT-FAMILY: "Times New Roman", Times, serif; FONT-WEIGHT: bold
}
A:link {
	TEXT-DECORATION: none
}
A:visited {
	TEXT-DECORATION: none
}
A:hover {
	TEXT-DECORATION: none
}
A:active {
	TEXT-DECORATION: none
}
BODY {
}

#container {
	WIDTH: 1024px; MARGIN: 0px auto;
}

</style>

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MUVA</title>

    <!-- Bootstrap -->
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/template.css" rel="stylesheet">
    <script type="text/javascript"><!--
    function obfuscate( domain, name ) { document.write('<a href="mai' +
        'lto:' + name + '@' + domain + '">' + name + '@' + domain + '</' + 'a>'); }
    // --></script>

</head>

<meta name="GENERATOR" content="MSHTML 11.00.10570.1001"></head>
<body onLoad=" ">
<div id="container" class="STYLE37">


<!--<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation" style="background:#7B7B7B">-->
<!--        &lt;!&ndash; Brand and toggle get grouped for better mobile display &ndash;&gt;-->
<!--    <div class="container">-->
<!--        &lt;!&ndash; Collect the nav links, forms, and other content for toggling &ndash;&gt;-->
<!--        <div class="collapse navbar-collapse navbar-ex1-collapse">-->
<!--        <ul class="nav navbar-nav">-->
<!--            <li><a href="index.html">Home</a></li>-->
<!--            <li><a href="index.html#abstract">Abstract</a></li>-->
<!--            <li><a href="index.html#framework">Framework</a></li>-->
<!--            <li><a href="index.html#overview">Overview</a></li>-->
<!--            <li><a href="index.html#example">Example</a></li>-->
<!--            <li><a href="index.html#code">Code</a></li>-->
<!--            <li><a href="index.html#citation">Citation</a></li>-->
<!--            <li><a href="index.html#contact">Contact</a></li>-->
<!--        </ul>-->
<!--        </div>-->

<!--    </div>-->
<!--</nav>-->



<p class="styleTittle" align="center">MUVA: A New Large-Scale Benchmark for Multi-view Amodal Instance Segmentation in the Shopping Scenario</p>

<p align="center"> <font size = 4><i>International Conference on Computer Vision (ICCV), 2023</i> </font> </p>


<!--<p align="center"><span class="STYLEAuthor">-->
<!--    <a href="https://zhixuanli.github.io/">Zhixuan Li</a><sup>1</sup>, &nbsp;&nbsp;-->
<!--    Weining Ye</a><sup>1</sup>, &nbsp;&nbsp;-->
<!--    <a href="https://scholar.google.com/citations?hl=en&user=6kh0YcEAAAAJ">Juan Terven</a><sup>2</sup>, &nbsp;&nbsp;-->
<!--    <a href="https://www.linkedin.com/in/zachary-bennett-b3199546/">Zachary Bennett</a><sup>2</sup>, &nbsp;&nbsp; <br>-->
<!--    <a href="https://www.linkedin.com/in/ying-zheng-1b9070a/">Ying Zheng</a><sup>2</sup>, &nbsp;&nbsp;-->
<!--    <a href="https://zhixuanli.github.io/">Tingting Jiang</a><sup>1</sup>, &nbsp;&nbsp;-->
<!--    <a href="https://zhixuanli.github.io/">Tiejun Huang</a><sup>1</sup>-->
<!--    </span>-->
<!--    </p>-->

<p align="center"><span class="STYLEAuthor">
    <a href="https://zhixuanli.github.io/">Zhixuan Li</a>, &nbsp;&nbsp;
    Weining Ye</a>, &nbsp;&nbsp;
    <a href="https://scholar.google.com/citations?hl=en&user=6kh0YcEAAAAJ">Juan Terven</a>, &nbsp;&nbsp;
    <a href="https://www.linkedin.com/in/zachary-bennett-b3199546/">Zachary Bennett</a>, &nbsp;&nbsp; <br>
    <a href="https://www.linkedin.com/in/ying-zheng-1b9070a/">Ying Zheng</a>, &nbsp;&nbsp;
    <a href="https://zhixuanli.github.io/">Tingting Jiang</a>, &nbsp;&nbsp;
    <a href="https://zhixuanli.github.io/">Tiejun Huang</a>
    </span>
    </p>

<p class="styleAffiliation" align="center">
<!--    1. <br>-->
<!--    2. <br>-->
<!--    3. <br>-->
<br>


<p class="STYLESection" align="center">Abstract</p>
<!--<div class="container">-->
<a name="abstract"></a>

<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
<div class="project-page well" >
<p class="styleAbstract" align="justify">
    Amodal Instance Segmentation (AIS) endeavors to accurately deduce complete object shapes that are partially or fully occluded.
    However, the inherent ill-posed nature of single-view datasets poses challenges in determining occluded shapes.
    A multi-view framework may help alleviate this problem, as humans often adjust their perspective when encountering occluded objects.
    At present, this approach has not yet been explored by existing methods and datasets.
    To bridge this gap, we propose a new task called <b>M</b>ulti-view <b>A</b>modal <b>I</b>nstance <b>S</b>egmentation (MAIS) and introduce the MUVA dataset,
    the first <b>MU</b>lti-<b>V</b>iew <b>A</b>IS dataset that takes the shopping scenario as instantiation.
    MUVA provides comprehensive annotations, including multi-view amodal/visible segmentation masks, 3D models, and depth maps, making it the largest image-level AIS dataset in terms of both the number of images and instances. Additionally, we propose a new method for aggregating representative features across different instances and views, which demonstrates promising results in accurately predicting occluded objects from one viewpoint by leveraging information from other viewpoints. Besides, we also demonstrate that MUVA can benefit the AIS task in real-world scenarios.
</p>
</div>
<br><br>
<!--</div>-->



<!--<div class="container">-->
<a name="framework"></a>
<p class="STYLESection" align="center">Targeting at the ill-posed problem in the AIS task</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0" style="margin: 0 auto;" >
  <tbody>
  <tr>
    <th scope="col"><img src="./figures/introduction.jpg" width="98%" style="margin: 0 auto;"></th></tr></tbody></table>
<p class="styleCaption" align="justify">
    Fig.1: Comparison of the impact of ill-posed problems on amodal prediction in single-view and multi-view input settings.
    (a) In single-view input, ambiguity arises due to multiple candidates for the occluded object.
    (b) Multi-view input helps alleviate ambiguity and improves amodal prediction accuracy.
</p>
<br><br>
<!--</div>-->



<!--<div class="container">-->
<a name="overview"></a>
<p class="STYLESection" align="center">Dataset Generation Pipeline</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./figures/dataset_generation_pipeline.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify">
    Fig. 2: The pipeline of dataset generation.
    (a) For each object, 2D images are captured from up, down, left, right, front, and back, respectively. Then 3D artists use the collected images to reconstruct the 3D models.
    (b) For each scene, 3D models are randomly selected and placed with different amounts and orientations. (c) For each scene, six views are used to capture the data, including the RGB images and various annotations.
</p>
<br><br>
<!--</div>-->

<!--<div class="container">-->
<a name="overview"></a>
<p class="STYLESection" align="center">Datasets Comparison</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./figures/datasets_comparison.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify">
    Tab 1: Comparison with existing amodal instance segmentation datasets. # means the number of this item.
    Bold numbers denote the largest one in each column among image-level datasets.
</p>
<br><br>
<!--</div>-->


<!--<div class="container">-->
<a name="example"></a>
<p class="STYLESection" align="center">Segmentation Results Comparison</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<table width="100%" border="0">
  <tbody>
  <tr>
    <th scope="col"><img src="./figures/exp_methods_compare.jpg" width="98%"></th></tr></tbody></table>
<p class="styleCaption" align="justify">
    Fig 3: Visualization comparisons between BCNet and ours on MUVA, trained with one viewpoint (1V) and six viewpoints (6V).
    For masks, different colors denote different instances, and the same instance in different angles has the same color. Red circles indicate
    regions should be focused. Zoom in for a better view. In the first and third columns, even if a bottle is severely occluded, it can be predicted
    accurately by our method. Moreover, our method trained with 6 views performs better than training with a single view.
</p>
<br><br>
<!--</div>-->


<!--<div class="container">-->
<a name="code"></a>
<p class="STYLESection" align="center">MUVA Dataset</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">

<p class="STYLEAuthor" align="center">
    Dataset Download Link (Coming soon...) &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp; &nbsp;&nbsp;
    Paper Link (Coming soon...)  <br>
<!--<a href="">Google Drive</a>-->
<!--<a href="">Aliyun Drive</a>-->
</p><br>


<a name="citation"></a>
<p class="STYLESection" align="center">Citation</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
<p class="STYLECitation">

<p class="lead">
    If you find this useful in your work, please consider citing the following reference:
    <div class="highlight">
    <pre> <code>
    @InProceedings{Li_2023ICCV_MUVA,
        author    = {Li, Zhixuan and Ye, Weining and Terven, Juan and Bennett, Zachary and Zheng, Ying and Jiang, Tingting and Huang, Tiejun},
        title     = {MUVA: A New Large-Scale Benchmark for Multi-view Amodal Instance Segmentation in the Shopping Scenario},
        booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
        month     = {October},
        year      = {2023}
    }
</code> </pre>
<!--    pages     = {XX-XX}-->

<a name="contact"></a>
<p class="STYLESection" align="center">Contact</p>
<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 3px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
</div>

<p class="STYLEAuthor">
If you have any question regarding this work, please send email to
<a href="zhixuanli@pku.edu.cn">zhixuanli@pku.edu.cn</a>.
</p>
<br><br>


<tr>
    <td style="padding:0px">
        <br>
        <p style="text-align:right;font-size:small;">thanks for the template of <a href="https://shiyuchengtju.github.io/index.html">Yucheng Shi</a>
        </p>
    </td>
</tr>

</pre></div>
</body></html>
